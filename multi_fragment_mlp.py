# -*- coding: utf-8 -*-
"""Multi fragment MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/147aVFpn4WY5NQvKrSbLPD8fTd2Rvnjeu
"""

import numpy as np

# ReLU activation function
def relu(z):
    return np.maximum(0, z)

# ReLU derivative for backpropagation
def relu_derivative(a):
    return (a > 0).astype(float)

# MLP class with ReLU activation and residual connections
class MLP:
    def __init__(self, input_size, hidden_size, output_size, support_hidden_size, learning_rate=0.01):
        # Main dataset weights with random initialization
        self.W1_main = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)  # He initialization
        self.b1_main = np.zeros((1, hidden_size))
        self.W2_main = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)  # He initialization
        self.b2_main = np.zeros((1, output_size))

        # Support datasets weights with random initialization
        self.W1_support1 = np.random.randn(input_size, support_hidden_size) * np.sqrt(2. / input_size)  # He initialization
        self.b1_support1 = np.zeros((1, support_hidden_size))
        self.W1_support2 = np.random.randn(input_size, support_hidden_size) * np.sqrt(2. / input_size)  # He initialization
        self.b1_support2 = np.zeros((1, support_hidden_size))
        self.W1_support3 = np.random.randn(input_size, support_hidden_size) * np.sqrt(2. / input_size)  # He initialization
        self.b1_support3 = np.zeros((1, support_hidden_size))

        # Cross connections weights with random initialization
        self.W_support_to_main = np.random.randn(3 * support_hidden_size, hidden_size) * np.sqrt(2. / (3 * support_hidden_size))  # He initialization

        self.learning_rate = learning_rate

    def forward(self, X_main, X_support1, X_support2, X_support3):
        # Forward pass for the main dataset with residual connection
        Z1_main = np.dot(X_main, self.W1_main) + self.b1_main
        A1_main = relu(Z1_main)

        # Residual connection
        A1_main_residual = A1_main

        # Forward pass for the support datasets
        Z1_support1 = np.dot(X_support1, self.W1_support1) + self.b1_support1
        A1_support1 = relu(Z1_support1)

        Z1_support2 = np.dot(X_support2, self.W1_support2) + self.b1_support2
        A1_support2 = relu(Z1_support2)

        Z1_support3 = np.dot(X_support3, self.W1_support3) + self.b1_support3
        A1_support3 = relu(Z1_support3)

        # Combine support dataset outputs and forward them to the main dataset
        combined_support_output = np.concatenate((A1_support1, A1_support2, A1_support3), axis=1)
        Z_support_to_main = np.dot(combined_support_output, self.W_support_to_main)
        A1_combined = A1_main_residual + Z_support_to_main  # Combine support with the main

        # Residual connection added to combined output
        A1_combined_residual = A1_combined

        # Output layer for the main dataset
        Z2_main = np.dot(A1_combined_residual, self.W2_main) + self.b2_main
        A2_main = relu(Z2_main)

        return A2_main, A1_main_residual, combined_support_output

    def backward(self, X_main, Y, A2_main, A1_main_residual, combined_support_output):
        m = X_main.shape[0]

        # Backpropagation for the main dataset
        dZ2_main = A2_main - Y
        dW2_main = np.dot(A1_main_residual.T, dZ2_main) / m
        db2_main = np.sum(dZ2_main, axis=0, keepdims=True) / m

        dA1_combined = np.dot(dZ2_main, self.W2_main.T)
        dW_support_to_main = np.dot(combined_support_output.T, dA1_combined) / m

        dZ1_main_residual = dA1_combined * relu_derivative(A1_main_residual)
        dW1_main = np.dot(X_main.T, dZ1_main_residual) / m
        db1_main = np.sum(dZ1_main_residual, axis=0, keepdims=True) / m

        # Update main dataset weights and biases
        self.W1_main -= self.learning_rate * dW1_main
        self.b1_main -= self.learning_rate * db1_main
        self.W2_main -= self.learning_rate * dW2_main
        self.b2_main -= self.learning_rate * db2_main

        # Update cross-connections weights
        self.W_support_to_main -= self.learning_rate * dW_support_to_main

        # Backpropagation for support datasets (only bias updates, no weight updates)
        dA1_support_combined = np.dot(dA1_combined, self.W_support_to_main.T)

        dZ1_support1 = dA1_support_combined[:, :self.W1_support1.shape[1]] * relu_derivative(combined_support_output[:, :self.W1_support1.shape[1]])
        db1_support1 = np.sum(dZ1_support1, axis=0, keepdims=True) / m

        dZ1_support2 = dA1_support_combined[:, self.W1_support1.shape[1]:2*self.W1_support1.shape[1]] * relu_derivative(combined_support_output[:, self.W1_support1.shape[1]:2*self.W1_support1.shape[1]])
        db1_support2 = np.sum(dZ1_support2, axis=0, keepdims=True) / m

        dZ1_support3 = dA1_support_combined[:, 2*self.W1_support1.shape[1]:] * relu_derivative(combined_support_output[:, 2*self.W1_support1.shape[1]:])
        db1_support3 = np.sum(dZ1_support3, axis=0, keepdims=True) / m

        # Update support datasets biases only
        self.b1_support1 -= self.learning_rate * db1_support1
        self.b1_support2 -= self.learning_rate * db1_support2
        self.b1_support3 -= self.learning_rate * db1_support3

    def train(self, X_main, X_support1, X_support2, X_support3, Y, epochs=1000):
        for i in range(epochs):
            # Forward pass
            A2_main, A1_main_residual, combined_support_output = self.forward(X_main, X_support1, X_support2, X_support3)

            # Backward pass
            self.backward(X_main, Y, A2_main, A1_main_residual, combined_support_output)

            if i % 100 == 0:
                loss = np.mean(np.square(Y - A2_main))  # Mean squared error
                print(f"Epoch {i}, Loss: {loss:.4f}")

# Example usage
if __name__ == "__main__":
    # Initialize main and support datasets
    X_main = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR inputs for main
    Y = np.array([[0], [1], [1], [0]])  # XOR outputs

    # Support datasets
    X_support1 = np.random.randn(4, 2)
    X_support2 = np.random.randn(4, 2)
    X_support3 = np.random.randn(4, 2)

    # Initialize MLP
    mlp = MLP(input_size=2, hidden_size=4, output_size=1, support_hidden_size=3)

    # Train the model
    mlp.train(X_main, X_support1, X_support2, X_support3, Y, epochs=1000)

    # Predict on the main dataset
    predictions, _, _ = mlp.forward(X_main, X_support1, X_support2, X_support3)
    print("Predictions:")
    print(predictions)